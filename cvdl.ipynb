{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import FileLink\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch import autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from models.gatedconv import InpaintGCNet, InpaintDirciminator\n",
    "from models.unet_fastai_resblock import InpaintSANet, InpaintSADirciminator\n",
    "from models.loss import SNDisLoss, SNGenLoss, ReconLoss, NewLoss\n",
    "\n",
    "from util.logger import TensorBoardLogger\n",
    "from util.config import Config\n",
    "from data.fastai_new import InpaintDataset\n",
    "from util.evaluation import AverageMeter\n",
    "from evaluation import metrics\n",
    "from PIL import Image\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "config = Config('config/inpaint_places2_sagan.yml')\n",
    "logger = logging.getLogger(__name__)\n",
    "time_stamp = time.strftime('%Y%m%d%H%M', time.localtime(time.time()))\n",
    "log_dir = 'model_logs/{}_{}'.format(time_stamp, config.LOG_DIR)\n",
    "result_dir = 'result_logs/{}_{}'.format(time_stamp, config.LOG_DIR)\n",
    "tensorboardlogger = TensorBoardLogger(log_dir)\n",
    "cuda0 = torch.device('cuda:{}'.format(config.GPU_ID))\n",
    "cpu0 = torch.device('cpu')\n",
    "\n",
    "def logger_init():\n",
    "    \"\"\"\n",
    "    Initialize the logger to some file.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logfile = 'logs/{}_{}.log'.format(time_stamp, config.LOG_DIR)\n",
    "    fh = logging.FileHandler(logfile, mode='w')\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mine': '/home/xudejia/inpainting/data/train_mask_list.txt'}\n",
      "{'val': '/home/xudejia/inpainting/data/val_mask_list.txt'}\n"
     ]
    }
   ],
   "source": [
    "    logger_init()\n",
    "    dataset_type = config.DATASET\n",
    "    batch_size = config.BATCH_SIZE\n",
    "train_dataset = InpaintDataset(config.DATA_FLIST[dataset_type][0],\\\n",
    "                                      {mask_type:config.DATA_FLIST[config.MASKDATASET][mask_type][0] for mask_type in config.MASK_TYPES}, \\\n",
    "                                      resize_shape=tuple(config.IMG_SHAPES), random_bbox_shape=config.RANDOM_BBOX_SHAPE, \\\n",
    "                                      random_bbox_margin=config.RANDOM_BBOX_MARGIN,\n",
    "                                      random_ff_setting=config.RANDOM_FF_SETTING)\n",
    "val_dataset = InpaintDataset(config.DATA_FLIST[dataset_type][1],\\\n",
    "                                    {mask_type:config.DATA_FLIST[config.MASKDATASET][mask_type][1] for mask_type in ('val',)}, \\\n",
    "                                    resize_shape=tuple(config.IMG_SHAPES), random_bbox_shape=config.RANDOM_BBOX_SHAPE, \\\n",
    "                                    random_bbox_margin=config.RANDOM_BBOX_MARGIN,\n",
    "                                    random_ff_setting=config.RANDOM_FF_SETTING,val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Finish the dataset initialization.\n"
     ]
    }
   ],
   "source": [
    "db = DataBunch.create(train_ds=train_dataset, valid_ds=val_dataset, bs=8, val_bs=8,num_workers=16, pin_memory=True)\n",
    "logger.info(\"Finish the dataset initialization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncnt = 0\\nfor w in db.train_ds:\\n    cnt = cnt + 1\\n    print('?', len(w))\\n    print(len(w[0]))\\n    for x in w[0]:\\n        print(x)\\n    if (cnt > 2):\\n        \\n        break\\nprint(cnt)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cnt = 0\n",
    "for w in db.train_ds:\n",
    "    cnt = cnt + 1\n",
    "    print('?', len(w))\n",
    "    print(len(w[0]))\n",
    "    for x in w[0]:\n",
    "        print(x)\n",
    "    if (cnt > 2):\n",
    "        \n",
    "        break\n",
    "print(cnt)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Define the Network Structure and Losses\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nif config.MODEL_RESTORE != \\'\\':\\n        whole_model_path = \\'model_logs/{}\\'.format( config.MODEL_RESTORE)\\n        nets = torch.load(whole_model_path)\\n        netG_state_dict, netD_state_dict = nets[\\'netG_state_dict\\'], nets[\\'netD_state_dict\\']\\n        netG.load_state_dict(netG_state_dict)\\n        netD.load_state_dict(netD_state_dict)\\n        logger.info(\"Loading pretrained models from {} ...\".format(config.MODEL_RESTORE))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"Define the Network Structure and Losses\")\n",
    "netG = InpaintSANet()\n",
    "# netD = InpaintSADirciminator()\n",
    "\"\"\"\n",
    "if config.MODEL_RESTORE != '':\n",
    "        whole_model_path = 'model_logs/{}'.format( config.MODEL_RESTORE)\n",
    "        nets = torch.load(whole_model_path)\n",
    "        netG_state_dict, netD_state_dict = nets['netG_state_dict'], nets['netD_state_dict']\n",
    "        netG.load_state_dict(netG_state_dict)\n",
    "        netD.load_state_dict(netD_state_dict)\n",
    "        logger.info(\"Loading pretrained models from {} ...\".format(config.MODEL_RESTORE))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLoss = ReconLoss(*(config.L1_LOSS_ALPHA))\n",
    "GLoss = SNGenLoss(config.GAN_LOSS_ALPHA)\n",
    "DLoss = SNDisLoss()\n",
    "NLoss = NewLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SaveModelCallback(learn, every='improvement', monitor='accuracy', name='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyLoss(out, img, masks):\n",
    "    \"\"\"\n",
    "    print(len(out))\n",
    "    print(len(target))\n",
    "    print(len(ww))\n",
    "    for x in out:\n",
    "        print(x.shape)\n",
    "    for x in target:\n",
    "        print(x.shape)\n",
    "    for x in ww:\n",
    "        print(x.shape)\n",
    "    \"\"\"\n",
    "    coarse_imgs, refined, mixed = out\n",
    "    # img, masks = target\n",
    "    complete_imgs = mixed\n",
    "    r_loss = RLoss(img, coarse_imgs, mixed, masks)\n",
    "    n_loss = NLoss(coarse_imgs, refined, mixed, img)\n",
    "    return r_loss + n_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2photo(imgs):\n",
    "    # torch.Size([3, 256, 256])\n",
    "    # return ((imgs+1)*127.5).detach().cpu().numpy()\n",
    "    return ((imgs+1)/2).detach().cpu()\n",
    "    # return ((imgs+1)*127.5).transpose(0, 1).transpose(1, 2).detach().cpu().numpy().astype(np.uint8)\n",
    "    # return ((imgs+1)*127.5).transpose(1,2).transpose(2,3).detach().cpu().numpy()\n",
    "class TensorboardLogger(Callback):\n",
    "    \"\"\"\n",
    "    A general Purpose Logger for TensorboardX\n",
    "    Also save a .txt file for the important parts\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learner, log_name, cfgtxt, del_existing=False, histogram_freq=100):\n",
    "        \"\"\"\n",
    "        Learner is the ConvLearner\n",
    "        log_name: name of the log directory to be formed. Will be input\n",
    "        for each run\n",
    "        cfgtxt: HyperParams\n",
    "        del_existing: To run the experiment from scratch and remove previous logs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.learn = learner\n",
    "        self.model = learner.model\n",
    "        self.md = learner.data\n",
    "\n",
    "        self.metrics_names = [\"validation_loss\"]\n",
    "        self.metrics_names += [m.__name__ for m in learner.metrics]\n",
    "\n",
    "        self.best_met = 0\n",
    "\n",
    "        self.histogram_freq = histogram_freq\n",
    "        self.cfgtxt = cfgtxt\n",
    "\n",
    "        path = Path(self.md.path) / \"model_logs\"\n",
    "        self.log_name = log_name\n",
    "        self.log_dir = path / log_name\n",
    "\n",
    "        self.init_logs(self.log_dir, del_existing)\n",
    "        self.init_tb_writer()\n",
    "        self.init_txt_writer(path, log_name)\n",
    "\n",
    "    def init_logs(self, log_dir, del_existing):\n",
    "        if log_dir.exists():\n",
    "            if del_existing:\n",
    "                print(f'removing existing log with same name {log_dir.stem}')\n",
    "                shutil.rmtree(self.log_dir)\n",
    "\n",
    "    def init_tb_writer(self):\n",
    "        self.writer = SummaryWriter(\n",
    "            comment='main_mdl', log_dir=str(self.log_dir))\n",
    "        self.writer.add_text('HyperParams', self.cfgtxt)\n",
    "\n",
    "    def init_txt_writer(self, path, log_name):\n",
    "        self.fw_ = path / f'{log_name}.txt'\n",
    "        self.str_form = '{} \\t {} \\t '\n",
    "        for m in self.metrics_names:\n",
    "            self.str_form += '{} \\t '\n",
    "        self.str_form += '\\n'\n",
    "        self.out_str = self.str_form.format(\n",
    "            'epoch', 'trn_loss', *self.metrics_names)\n",
    "\n",
    "        with open(self.fw_, 'w') as f:\n",
    "            f.write(self.cfgtxt)\n",
    "            f.write('\\n')\n",
    "            f.write(self.out_str)\n",
    "\n",
    "    def on_batch_end(self, **kwargs):\n",
    "        self.trn_loss = kwargs['last_loss']\n",
    "        num_batch = kwargs['num_batch']\n",
    "        self.writer.add_scalar(\n",
    "            'trn_loss_batch', self.trn_loss, num_batch)\n",
    "        last_output = kwargs['last_output']\n",
    "        last_target = kwargs['last_target']\n",
    "        epoch = kwargs['epoch']\n",
    "        iteration = kwargs['iteration']\n",
    "        if iteration % 5 == 0:\n",
    "            \"\"\"\n",
    "            print(len(last_target[0]))\n",
    "            for x in last_target[0]:\n",
    "                print(x.shape)\n",
    "            \"\"\"\n",
    "            imgs, masks = last_target[0][0].detach(), last_target[1][0].detach()\n",
    "            coarse_imgs, refined, complete_imgs = last_output[0][0].detach(), last_output[1][0].detach(), last_output[2][0].detach()\n",
    "            # torch.Size([3, 256, 256])\n",
    "            img = img2photo(torch.cat([imgs * (1 - masks) + masks, refined, imgs * masks, complete_imgs, imgs], dim=2))\n",
    "            self.writer.add_image('train/whole_imgs%d'%epoch, img, iteration)\n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        metrics = kwargs['last_metrics']\n",
    "        epoch = kwargs['epoch']\n",
    "        trn_loss = kwargs['smooth_loss']\n",
    "        self.writer.add_scalar('trn_loss', trn_loss, epoch)\n",
    "\n",
    "        for val, name in zip(metrics, self.metrics_names):\n",
    "            self.writer.add_scalar(name, val, epoch)\n",
    "\n",
    "        self.file_write(self.str_form.format(epoch,\n",
    "                                             self.trn_loss, *metrics))\n",
    "\n",
    "        m = metrics[1]\n",
    "        if m > self.best_met:\n",
    "            self.best_met = m\n",
    "            self.learn.save(self.log_name)\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        self.writer.add_text('Total Epochs', str(kwargs['epoch']))\n",
    "        self.writer.close()\n",
    "        self.file_write(f'Epochs done, {kwargs[\"epoch\"]}')\n",
    "\n",
    "    def file_write(self, outstr):\n",
    "        with open(self.fw_, 'a') as f:\n",
    "            f.write(outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSNR(Callback):\n",
    "    \"Wrap a `func` in a callback for metrics computation.\"\n",
    "    def __init__(self):\n",
    "        # If it's a partial, use func.func\n",
    "        # name = getattr(func,'func',func).__name__\n",
    "        self.name = 'PSNR'\n",
    "        self.__name__ = 'PSNR'\n",
    "        self.func = metrics['ppsnr']\n",
    "        # self.func, self.name = func, name\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        \"Set the inner value to 0.\"\n",
    "        self.val, self.count = 0.,0\n",
    "\n",
    "    def on_batch_end(self, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            ww = kwargs['last_output']\n",
    "            aa = kwargs['last_target']\n",
    "            last_output = ww[-1]\n",
    "            last_target = aa[0]\n",
    "            \"Update metric computation with `last_output` and `last_target`.\"\n",
    "            if not is_listy(last_target): last_target=[last_target]\n",
    "            self.count += last_target[0].size(0)\n",
    "            \"\"\"\n",
    "            print(len(last_output))\n",
    "            for x in last_output:\n",
    "                print(x.shape)\n",
    "            \"\"\"\n",
    "            # val = self.func(last_output, *last_target)\n",
    "            val = self.func(*last_target, last_output)\n",
    "            self.val += last_target[0].size(0) * val\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        \"Set the final result in `last_metrics`.\"\n",
    "        return add_metrics(last_metrics, self.val/self.count)\n",
    "class SSIM(Callback):\n",
    "    \"Wrap a `func` in a callback for metrics computation.\"\n",
    "    def __init__(self):\n",
    "        # If it's a partial, use func.func\n",
    "        # name = getattr(func,'func',func).__name__\n",
    "        self.name = 'SSIM'\n",
    "        self.__name__ = 'SSIM'\n",
    "        self.func = metrics['sssim']\n",
    "        # self.func, self.name = func, name\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        \"Set the inner value to 0.\"\n",
    "        self.val, self.count = 0.,0\n",
    "\n",
    "    def on_batch_end(self, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            ww = kwargs['last_output']\n",
    "            aa = kwargs['last_target']\n",
    "            last_output = ww[-1]\n",
    "            last_target = aa[0]\n",
    "            # last_output = last_output[0]\n",
    "            \"Update metric computation with `last_output` and `last_target`.\"\n",
    "            if not is_listy(last_target): last_target=[last_target]\n",
    "            self.count += last_target[0].size(0)\n",
    "            # val = self.func(last_output, *last_target)\n",
    "            val = self.func(*last_target, last_output)\n",
    "            self.val += last_target[0].size(0) * val\n",
    "        # self.val += last_target[0].size(0) * val.detach().cpu()\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        \"Set the final result in `last_metrics`.\"\n",
    "        return add_metrics(last_metrics, self.val/self.count)\n",
    "psnr = PSNR()\n",
    "ssim = SSIM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, decay = config.LEARNING_RATE, config.WEIGHT_DECAY\n",
    "learn = Learner(db, netG, loss_func=MyLoss, metrics=[psnr, ssim], callback_fns=[partial(EarlyStoppingCallback, monitor='PSNR', min_delta=0.01, patience=20)], model_dir='./weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.validate()\n",
    "# [6.939414, 9.518688486670799, tensor(0.2730, device='cuda:0')]\n",
    "# loss, psnr, ssim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load('best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(1, lr=1e-2, callback_fns=[SaveModelCallback(learn, every='improvement', monitor='psnr', name='best'), TensorboardLogger(learn, \"fastai-1\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='39' class='' max='50', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      78.00% [39/50 9:23:52<2:39:02]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>PSNR</th>\n",
       "      <th>SSIM</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.774677</td>\n",
       "      <td>3.567409</td>\n",
       "      <td>13.320558</td>\n",
       "      <td>0.459878</td>\n",
       "      <td>14:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.897248</td>\n",
       "      <td>2.820148</td>\n",
       "      <td>16.848862</td>\n",
       "      <td>0.679458</td>\n",
       "      <td>14:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.457076</td>\n",
       "      <td>2.259952</td>\n",
       "      <td>19.819332</td>\n",
       "      <td>0.794687</td>\n",
       "      <td>14:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.210426</td>\n",
       "      <td>2.028085</td>\n",
       "      <td>21.286739</td>\n",
       "      <td>0.829802</td>\n",
       "      <td>14:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.185605</td>\n",
       "      <td>1.991203</td>\n",
       "      <td>21.488320</td>\n",
       "      <td>0.844458</td>\n",
       "      <td>14:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.082666</td>\n",
       "      <td>1.956951</td>\n",
       "      <td>21.617173</td>\n",
       "      <td>0.849429</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.060935</td>\n",
       "      <td>1.981109</td>\n",
       "      <td>21.608636</td>\n",
       "      <td>0.854147</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.042943</td>\n",
       "      <td>1.934303</td>\n",
       "      <td>21.695139</td>\n",
       "      <td>0.855637</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.043468</td>\n",
       "      <td>1.910449</td>\n",
       "      <td>21.760363</td>\n",
       "      <td>0.856851</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.998603</td>\n",
       "      <td>1.880546</td>\n",
       "      <td>21.781602</td>\n",
       "      <td>0.861354</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.049878</td>\n",
       "      <td>1.872869</td>\n",
       "      <td>21.803874</td>\n",
       "      <td>0.861469</td>\n",
       "      <td>14:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.989869</td>\n",
       "      <td>1.856796</td>\n",
       "      <td>21.901652</td>\n",
       "      <td>0.862469</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.963712</td>\n",
       "      <td>1.864910</td>\n",
       "      <td>21.809385</td>\n",
       "      <td>0.860936</td>\n",
       "      <td>14:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.877074</td>\n",
       "      <td>1.831149</td>\n",
       "      <td>21.870588</td>\n",
       "      <td>0.863850</td>\n",
       "      <td>14:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.946562</td>\n",
       "      <td>1.843989</td>\n",
       "      <td>21.715325</td>\n",
       "      <td>0.863093</td>\n",
       "      <td>14:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.912587</td>\n",
       "      <td>1.848289</td>\n",
       "      <td>21.745562</td>\n",
       "      <td>0.863078</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.882351</td>\n",
       "      <td>1.869160</td>\n",
       "      <td>21.565405</td>\n",
       "      <td>0.862765</td>\n",
       "      <td>14:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.851913</td>\n",
       "      <td>1.870928</td>\n",
       "      <td>21.701432</td>\n",
       "      <td>0.862725</td>\n",
       "      <td>14:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.828466</td>\n",
       "      <td>1.798459</td>\n",
       "      <td>21.917992</td>\n",
       "      <td>0.865848</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.797417</td>\n",
       "      <td>1.833927</td>\n",
       "      <td>21.806079</td>\n",
       "      <td>0.865058</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.807789</td>\n",
       "      <td>1.830521</td>\n",
       "      <td>21.762186</td>\n",
       "      <td>0.864426</td>\n",
       "      <td>14:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.818383</td>\n",
       "      <td>1.832993</td>\n",
       "      <td>21.640341</td>\n",
       "      <td>0.864178</td>\n",
       "      <td>14:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.767449</td>\n",
       "      <td>1.819502</td>\n",
       "      <td>21.709037</td>\n",
       "      <td>0.864588</td>\n",
       "      <td>14:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.747918</td>\n",
       "      <td>1.849747</td>\n",
       "      <td>21.684199</td>\n",
       "      <td>0.863614</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.760757</td>\n",
       "      <td>1.846671</td>\n",
       "      <td>21.404973</td>\n",
       "      <td>0.860964</td>\n",
       "      <td>14:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.703812</td>\n",
       "      <td>1.879781</td>\n",
       "      <td>21.514838</td>\n",
       "      <td>0.862573</td>\n",
       "      <td>14:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.696451</td>\n",
       "      <td>1.857753</td>\n",
       "      <td>21.599132</td>\n",
       "      <td>0.863427</td>\n",
       "      <td>14:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.709152</td>\n",
       "      <td>1.853684</td>\n",
       "      <td>21.577755</td>\n",
       "      <td>0.863268</td>\n",
       "      <td>14:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.652810</td>\n",
       "      <td>1.850007</td>\n",
       "      <td>21.576727</td>\n",
       "      <td>0.864090</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.645777</td>\n",
       "      <td>1.875765</td>\n",
       "      <td>21.411674</td>\n",
       "      <td>0.860809</td>\n",
       "      <td>14:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.630754</td>\n",
       "      <td>1.873401</td>\n",
       "      <td>21.664973</td>\n",
       "      <td>0.862908</td>\n",
       "      <td>14:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.633940</td>\n",
       "      <td>1.848060</td>\n",
       "      <td>21.499737</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>14:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.621976</td>\n",
       "      <td>1.876288</td>\n",
       "      <td>21.322041</td>\n",
       "      <td>0.860379</td>\n",
       "      <td>14:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.590955</td>\n",
       "      <td>1.871185</td>\n",
       "      <td>21.458695</td>\n",
       "      <td>0.862418</td>\n",
       "      <td>14:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.620944</td>\n",
       "      <td>1.831732</td>\n",
       "      <td>21.557768</td>\n",
       "      <td>0.863759</td>\n",
       "      <td>14:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.597464</td>\n",
       "      <td>1.865352</td>\n",
       "      <td>21.471441</td>\n",
       "      <td>0.861239</td>\n",
       "      <td>14:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.574790</td>\n",
       "      <td>1.888092</td>\n",
       "      <td>21.220114</td>\n",
       "      <td>0.859365</td>\n",
       "      <td>14:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.584575</td>\n",
       "      <td>1.873495</td>\n",
       "      <td>21.338993</td>\n",
       "      <td>0.860515</td>\n",
       "      <td>14:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.521801</td>\n",
       "      <td>1.840201</td>\n",
       "      <td>21.619190</td>\n",
       "      <td>0.864711</td>\n",
       "      <td>14:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:07<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xudejia/anaconda2/envs/py3.6/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/xudejia/anaconda2/envs/py3.6/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/xudejia/anaconda2/envs/py3.6/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with PSNR value: 13.320558254732047.\n",
      "Better model found at epoch 1 with PSNR value: 16.848861514467725.\n",
      "Better model found at epoch 2 with PSNR value: 19.81933218424781.\n",
      "Better model found at epoch 3 with PSNR value: 21.286739305405636.\n",
      "Better model found at epoch 4 with PSNR value: 21.488319772161258.\n",
      "Better model found at epoch 5 with PSNR value: 21.617173099175265.\n",
      "Better model found at epoch 7 with PSNR value: 21.695138751223475.\n",
      "Better model found at epoch 8 with PSNR value: 21.76036257810438.\n",
      "Better model found at epoch 9 with PSNR value: 21.781601624460567.\n",
      "Better model found at epoch 10 with PSNR value: 21.803873743328236.\n",
      "Better model found at epoch 11 with PSNR value: 21.901651671204533.\n",
      "Better model found at epoch 18 with PSNR value: 21.91799160941571.\n",
      "Epoch 39: early stopping\n"
     ]
    }
   ],
   "source": [
    "# lr = (1e-2)/2\n",
    "# 可能是 lr 变大之后才炸的..\n",
    "lr = (1e-4)\n",
    "NAME = \"fastai-unet-swish-concat\" # with new val\n",
    "# learn.fit_one_cycle(50, max_lr=slice(lr), callbacks=[SaveModelCallback(learn, every='improvement', monitor='PSNR', name='best'), TensorboardLogger(learn, NAME, cfgtxt=str(lr))])\n",
    "learn.fit(50, lr=(lr), callbacks=[SaveModelCallback(learn, every='improvement', monitor='PSNR', name='best'), TensorboardLogger(learn, NAME, cfgtxt=str(lr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
